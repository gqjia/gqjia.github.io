title: T5
date: 2022-04-18 10:17:11

论文地址：

[ReadPaper:Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://readpaper.com/paper/2981852735)

[arXiv:Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683)

代码地址：

 [google-research](https://github.com/google-research)/**[text-to-text-transfer-transformer](https://github.com/google-research/text-to-text-transfer-transformer)**

论文作者是 Colin Raffel 、Noam Shazeer 、 Adam Roberts

机构是无敌的谷歌

## 论文摘要

T5通过设计了一个统一的框架（将所有的NLP任务转换为文本到文本的生成任务）来研究预训练语言模型。研究的内容包括预训练任务、模型结构、数据集、迁移方法和其他因素。同时论文收集了一个 C4 数据集（Colossal Clean Crawled Corpus）。模型在摘要、问答、文本分类都取得了最好的效果。

由于预训练语言模型太过好用，各种预训练方法、数据集、微调策略等层出不穷，而论文就是对现有的技术进行分析和研究。这篇论文并不是提出一种新的方法，而是希望构建一个研究的基础，让研究者可以基于这个模型进行深入研究。

T5将所有的NLP任务转换为text-to-text的任务。通过这种统一的方法可以比较不同的预训练目标、数据集和其他因素的有效性，同时通过扩展模型和数据集来探索NLP的迁移学习的限制。

![image-20220418114026788](https://raw.githubusercontent.com/Moriarty12138/PictureBed/main/img/202204181140924.png)



## 模型结构

T5 的模型整体结构使用最原始的 Transformer 结构。

在 Layer Normalization 部分，模型使用最简单的 LayerNorm 策略，只对激活进行重新缩放，不会增加额外的偏置。在 Layer Normalization 层之后使用残差连接将每个子模块的输入增加到输出中。Dropout 应用在前馈网络、skip connection（？）、注意力权重和整个模块的输出（？）中。

解码器的自注意机制使用自回归注意力/因果自注意力（autoregressive or causal self-attention）（？）。

位置编码部分使用简化位置编码。每个 token 的嵌入（embedding）是一个标量。为了提高效率，模型在所有层之间共享位置嵌入。原文对于位置编码这部分的描述如下：

>   In this work, we use 32 embeddings for all of our models with ranges that increase in size logarithmically up to an offset of 128 beyond which we assign all relative positions to the same embedding.







## 参考资料

[^1]:[ReadPaper:Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://readpaper.com/paper/2981852735)
[^2]:[arXiv:Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683)
[^3]:[T5: 文本到文本的Transformer迁移学习 ](https://mp.weixin.qq.com/s?__biz=MzI4ODg3NDY2NQ==&mid=2247484599&idx=1&sn=2d2e8eb9d4214eb2342f27b71ebc8567&chksm=ec368d71db4104670c9eb32cc7ff9e75589ff176f764510287ef483e5ea99e7f1cb611154eb5&token=956863618&lang=zh_CN#rd)