---
title: 论文笔记 DoRA
date: 2024-02-22 14-09-05
---



DoRA arxiv: https://arxiv.org/abs/2402.09353

DoRA github: https://github.com/catid/dora

DoRA Kimi: https://papers.cool/arxiv/2402.09353

LoRA arxiv: https://arxiv.org/abs/2106.09685

LoRA github: https://github.com/microsoft/LoRA



在看 DoRA 之前需要先回顾一下 LoRA。



LoRA 通过在模型权重更新中加入 **低秩矩阵** 实现对大模型 低成本的高效微调。

<img src="https://raw.githubusercontent.com/gqjia/PictureBed/main/img/202402221355832.png" alt="image-20240219225230990" style="zoom:50%;" />

如上所示，LoRA 在 dense layer 外加两个矩阵，实现对 dense layer 的间接训练。在训练过程中，保持预训练权重不变，只对增加的权重进行训练。 

这一改进的思路来源于Aghajanyan 的 [论文](http://arxiv.org/abs/2012.13255) ，预训练语言模型具有较低的 内在维度，即使是投影到一个更小的子空间上让人可以进行高效的学习。因此作者假设权重在更新的过程中也具有较低的 内在秩 。也就是说对于权重更新 $W_0 + \Delta W$ 表示为 $W_0 + BA$ ，其中 $B \in R^{d \times r}$   $A \in R^{r \times k}$ 并且 $r \ll min(d, k)$ 。初始化时， $A$ 进行随机高斯初始化，$B$ 全设置为零。这样可以确保开始训练时 $\Delta W$ 为零。然后通过 $\frac{\alpha}{r}$ 对  $\Delta W x$ 进行缩放，其中 $\alpha$ 是与 $r$ 相关联的常数。

Transformer结构中，自注意力模型包括四个权重矩阵（$W_q, W_k, W_v, W_o$），MLP模块有两个。作者在 注意力层做了实验。

<img src="https://raw.githubusercontent.com/gqjia/PictureBed/main/img/202402221355710.png" alt="image-20240219234646660" style="zoom:50%;" />

在相同可训练参数下，同时调整 $W_q$、$ W_v$ 的效果最好。而且相比使用更高的秩、训练一个权重矩阵，使用更低的秩、训练多个权重矩阵 效果更好。

作者在 GPT-3 175B 上进行了实验，证明即使完整的秩高达12288，使用很低的秩就已经足够。



LoRA的优势如下：

1.   针对不同的小任务训练不同的 LoRA。可以在冻结共享模型权重时，替换不同LoRA 切换任务，降低存储和任务切换开销。
2.   训练更加高效，降低了硬件门槛。
3.   可以合并权重，推理时和全量微调一样。
4.   可以与其它微调方法结合使用。



DoRA 在 LoRA 的基础上，将预训练权重分解为 幅度 和 方向 。

<img src="https://raw.githubusercontent.com/gqjia/PictureBed/main/img/202402221355596.png" alt="image-20240219235636417" style="zoom:50%;" />

作者受到 salimans [论文](Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks)的启发，将权重矩阵重新参数化为幅度和方向，实现加速优化。拆分如下：
$$
W = m \frac{V}{||V||_c} = ||w||_c \frac{W}{||W||_c}
$$
其中，$m$ 是幅度向量，$V$​ 是 方向矩阵。

作者用VLBART进行实验，按照 Sung [论文](https://arxiv.org/abs/2112.06825)的发现，只对自注意力模块的 Q、V 权重矩阵挂载 LoRA 。



$W_0$ 和 $W_{FT}$ 之间幅度和方向变化可以表示为：
$$
\Delta M_{FT}^{t} = \frac{\sum_{n=1}^{k}{|m_{FT}^{n,t} - m_0^n|}}{k}
$$

$$
\Delta D_{FT}^{t} = \frac{\sum_{n=1}^{k}(1 - cos(V_{FT}^{n,t}, W_0^n))}{k}
$$

M是幅度，D是方向，方向计算余弦相似度。

统计 FT、LoRA、DoRA 不同层的数据如下：

<img src="https://raw.githubusercontent.com/gqjia/PictureBed/main/img/202402221356688.png" alt="image-20240220012125668" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/gqjia/PictureBed/main/img/202402221356623.png" alt="image-20240220012115307" style="zoom:50%;" />

LoRA在所有的中间步骤中都表现出一致性的趋势，而 FT 则表现出多样性的学习模式。而作者设计的 DoRA 更加接近 FT。



DoRA 对 M 进行全量微调，对 D 做 LoRA 微调。
$$
W' = \underline{m} \frac{V + \Delta V}{||V + \Delta V||_c} = \underline{m} \frac{W_0 + \underline{BA}}{||W_0 + \underline{BA}||_c}
$$


作者对 DoRA 做了梯度上的分析。这种分解可以将优化收益转移到$\Delta V$，增强了 LoRA 的稳定性。作者假设了两个场景S1 和 S2 ，其中 S1 在方向上的更新变化小于 S2的。DoRA 使得 S1 更新幅度大于 S2，但是 方向变化小于 S2 ，正如上图所示。 DoRA的更新模型相比 LoRA 更加接近 FT。





![image-20240222114845425](https://raw.githubusercontent.com/gqjia/PictureBed/main/img/202402221356521.png)

在常识推理任务上，DoRA 在 LLaMA-7B 上相比 LoRA 提高了 3.4%，LLaMA-13B上 1%。即使将秩减半，也分别超过 2.8%、1%。



![image-20240222115151546](https://raw.githubusercontent.com/gqjia/PictureBed/main/img/202402221356823.png)

在图像-文本理解任务上，超过1%，达到FT的水平。视频-文本理解任务上超过 2%。



![image-20240222115335279](https://raw.githubusercontent.com/gqjia/PictureBed/main/img/202402221356120.png)

在 LLaVA1.5-7B 上，DoRA 平均准确率超过 FT 和 LoRa（FT可能存在过拟合）。



![image-20240222134314978](https://raw.githubusercontent.com/gqjia/PictureBed/main/img/202402221356878.png)

DoRA 可以和其他LoRA 变体兼容。







