---
title: 【NLP】【CS224N】2.1 word2vec
date: 2018-11-03 19:42:31
---
来自S的视频。  

1. 拼写检查、关键字检索......
2. 文本挖掘
3. 文本分类
4. 机器翻译
5. 客服系统
6. 复杂对话系统

### 语言模型
p(s)=p(w1,w2,w3,w4,...,wn)  
=p(w1)p(w2|w1)p(w3|w1,w2)...p(wn|w1,w2,...,wn-1)  
p(S)被称为语言模型，即用来计算一个句子概率的模型。  
1.数据过于稀疏  
2.参数空间太大  


### N-gram模型
假设下一个词的出现依赖它前面的一个词：  
p(s)=p(w1,w2,w3,w4,...,wn)  
=p(w1)p(w2|w1)p(w3|w2)...p(wn|wn-1)  

假设下一个词的出现依赖它前面的两个词：  
p(s)=p(w1,w2,w3,w4,...,wn)  
=p(w1)p(w2|w1)p(w3|w1,w2)...p(wn|wn-2,wn-1)  

在实际问题中，并不指定跟前面所有的词都相关，而是只与前n个词相关。  
即N-gram模型  
解决了参数空间过大的问题。  

假设词典的大小是N，则模型参数的量级是(O(N^n))


### 词向量
word2vec  

### 神经网络模型
input layer --> projection layer --> hidden layer --> output layer  
训练样本：(context(w),w)包括前n-1个词分别的向量，假定每个词向量大小m  
投影层：(n-1) * m 首尾拼接起来的大向量  
输出：yw=(yw1,yw2,...,ywN)^T   
表示上下文context(w)时，下一个词恰好为词典中第i个词的概率  
归一化：p(w|context(w))=(e^(yw,iw)/(Σe^(yw,i)))  

神经网路模型的优势：  
对于N-gram模型：p(S1)>>p(S2)  
对于神经网络模型：p(S1)=p(S2)  

对与几个相近的句子  
只要语料库中出现其中一个，其他句子的概率也会相应的增大。  

### Hierarchical Softmax
![CBOW](/images/DL-images/cs224n-2-1-cbow.png)  
CBOW是Continuous Bag-of-Words Model的缩写，是一种根据上下文的词语预测当前词语出现概率的模型。  
![CBOW Model](/images/DL-images/cs224n-2-1-cbow-1.png)  
哈夫曼树：  
![Haffeman Tree](/images/DL-images/cs224n-2-1-hafferman-tree.png)  
![Haffeman Tree](/images/DL-images/cs224n-2-1-hafferman-tree-1.png)  
Logistic回归：  
![Logistic Regression](/images/DL-images/cs224n-2-1-cbow-logistic.png)  
Softmax相当于一个多分类的逻辑斯蒂回归。  
CBOW：  
![CBOW Model](/images/DL-images/cs224n-2-1-cbow-2.png)  
![CBOW Model](/images/DL-images/cs224n-2-1-cbow-3.png)  

CBOW的更新

![Skip-gram](/images/DL-images/cs224n-2-1-skipgram.png)  
