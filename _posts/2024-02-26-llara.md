---
title: LLaRA 技术报告
date: 2024-02-26 13-41-05
---



arxiv：https://arxiv.org/abs/2312.15503

github：https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md （未开源）





作者用 LLaMA2 来做文本的 embedding 的方法 LLaRA ，为此作者设计了两个任务 EBAE (Embedding-Based Auto-Encoding) 和 EBAR (Embedding-Based Auto-Regression) 。

![image-20240226105505878](https://gqjia-images-1254146217.cos.ap-nanjing.myqcloud.com/gqjia-post202402261125546.png)



文本 embedding 需要满足以下两个要求：

1.   需要表示全局上下文的语义
2.   有助于确定 query 和 doc 之间的关联程度

因此设计了两个训练的任务（其实也就是 prompt）：

>   “[Placeholder for input]  The original sentence:  ⟨\s⟩”

和

>   “[Placeholder for input]  The next sentence:  ⟨\s⟩”

也就是之前提到的 EBAE 和 EBAR 。

为节省算力，作者将两个任务合并，修改 attention mask 如下：

<img src="https://gqjia-images-1254146217.cos.ap-nanjing.myqcloud.com/gqjia-post202402261124718.png" alt="image-20240226112415685" style="zoom:50%;" />

SELF 就是 The original sentence，NEXT 就是 The next sentence 。



后续就是实验部分。

模型选择 LLaMA2 7B ，数据集采用 MS MARCO。

LoRA微调 10000 步，batch size 256，序列长度 1024，学习率 1e-5。使用了 ANN 困难负例进行对比学习。

<img src="https://gqjia-images-1254146217.cos.ap-nanjing.myqcloud.com/gqjia-post202402261240530.png" alt="image-20240226124037490" style="zoom:50%;" />

<img src="https://gqjia-images-1254146217.cos.ap-nanjing.myqcloud.com/gqjia-post202402261240379.png" alt="image-20240226124059357" style="zoom:50%;" />

MS MARCO段落检索上，与最接近的RepLLaMA相比，LLaRA在MS MARCO段落检索中的MRR@10提高了1.9%，在MS MARCO文档检索中的MRR@100提高了1.9%，在BEIR零样本检索中的NDCG@10提高了1.0%。

MS MARCO的文档检索上，LLaRA相比之前基于BERT的方法在MRR@100上提升了超过5%。主要是由于 LLaMA 支持更长的上下文。

在BEIR基准测试中的zero-shot评估上，与BERT基线相比，LLaRA在每个单独任务中都取得了更好的性能，最终导致平均性能的NDCG@10提升了16%。

