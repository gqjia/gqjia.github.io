---
title: DeepSeek LLM 技术文档
date: 2024-01-09 16-04-53
---


文章链接：https://arxiv.org/pdf/2401.02954.pdf
代码仓库链接：https://github.com/deepseek-ai/DeepSeek-LLM

## 1.简介

DeepSeek发布了6B和67B的两种模型。在预训练中使用了2T token，并在之后进行了SFT和DPO。

## 2.预训练



### 2.1 数据预处理

数据预处理包括去重、筛选和重新混合三个步骤。其中筛选过程考虑了语言和语义方面的质量评估。重新混合过程对数据在领域上的不平衡进行了调整。作者实现了BBPE tokenizer，并仿照Llama将数字切分处理。最终的vocabulary size被设置为102400。

### 2.2 模型结构

模型结构大体和Llama相似，使用了RMSNorm、SwiGLU、RoPE，并在67B模型中使用了GQA。但在层数上与Llama同级别模型有所不同。

### 2.3 超参数

作者使用了AdamW优化器，其中beta_1=0.9，beta_2=0.95，权重衰减为0.1。
学习率经过2000步预热后达到最大值，然后逐步衰减，在训练完80% token时减少为峰值的31.6%，在训练完90% token时减少为峰值10%。作者通过实验证实，虽然这种学习率设置使得损失函数下降的曲线与cosine scheduler不完全相同，但二者最终的表现是一致的。同时，这样做的好处是在不改变模型规模、只对训练规模进行调整时，之前的训练结果可以被复用。
训练过程中的梯度裁剪设置为1.0。

### 2.4 训练框架

作者使用了HAI-LLM框架，其中实现了并行训练相关的功能。Flash Attention和ZERO-1也得到了使用。

## 3. 对齐

作者共计收集了约150万条指令数据用于对齐，包含中文和英文。其中120万条数据关注helpfulness，30万条关注safety。

### 3.1 SFT

作者对7B模型进行了4个epochs的fine-tuning，但对67B模型只进行了2个epochs，因为67B模型达到上限较快。学习率分别为1e-5（7B）和5e-6（67B）。

### 3.2 DPO

作者收集了多语言的helpfulness和harmless相关的prompt，然后使用deepseek chat model生成回答，构建了DPO所使用的数据集。
作者训练了1个epoch，使用的学习率为5e-6，并使用学习率预热和cosine scheduler。作者发现DPO可以增强模型的生成能力，而且对标准评价指标不会产生很大影响。

## 4. 评测结果

评测过程包括针对公开数据集的评测、开放式生成任务的评测、留出测试集评测、和安全性评测。具体内容可以参见原文表格。