---
title: 谢谢你， BART ！
date: 2022-05-23 20:24:01
---









由于平行数据的缺乏导致文本风格迁移模型在保存内容方面成功率很低，作者尝试了在与训练语言模型 GPT-2 和 BART 进行微调。实验证明，预训练语言模型即使在数据有限的情况下也能提高内容保存能力。作者使用强化学习的策略，把风格和内容两方面作为奖励。实验证明模型在数据集上取得 SOTA 。

## 论文模型

![image-20220523202056657](https://raw.githubusercontent.com/Moriarty12138/PictureBed/main/img/202205232020061.png)

论文提出了一个在预训练语言模型基础上的强化学习框架。

